\chapter{Synthèse des résultats}

    \begin{table}[H]
        \centering
        \begin{tabular}{p{0.2\textwidth}|u|d|t|q} \toprule
            \textbf{Tâche} & $\mathtt S\left(T_1\right)$ & $\mathtt S\left(T_2\right)$ & $\mathtt S\left(T_3\right)$ & $\mathtt S\left(T_4\right)$
            \\
            \midrule
            \textbf{Score}& 0.52 & 0 56 & 0.50 & 0.50\\
            \bottomrule
        \end{tabular}
        \caption{Meilleurs scores obtenus sur chaque tâche}
    \end{table}

    \section{Tâche 1}
        La taille des données d'entrée rend la classification particulièrement longue si l'on
        utilise des modèles réce®nts. Ainsi, l'équipe n'a réalisé qu'une classification, sur les
        mêmes hypothèses que la tâche 2. Le score obtenu est plutôt faible, ce qui suggère
        qu'une augmentation de la taille des données d'entraînement est requise.

    \section{Tâche 2}\label{concTask2}
        On choisit d'aborder cette tâche en premier en raison de la petite taille de son dataset
        privé. En effet, on peut supposer que la variation d'un paramètre aura une plus forte sensibilité sur un petit ensemble.
        Sur un nombre raisonnable de datasets, les performances des classifieurs sont
        excellentes, sans que l'on ait étudié en détail la qualité des données d'entraînement.
        L'amélioration de cette qualité aurait sans doute permis d'augmenter significativement
        le score.

    \section{Tâche 3}
        Cette tâche n'a pas été abordée car elle nécessite de longs temps de calcul et un retour
        d'expérience de la tâche 4, impossible compte tenu du stade du projet auquel nous
        l'avons étudiée.

    \section{Tâche 4}
        Abordée très tardivement, cette tâche complexe aurait nécessité une prise de recul
        importante pour savoir comment exploiter les quelques résultats que nous avons pu en
        tirer. En effet, l'ensemble à classifier est différent des ensembles des tâches 1 et 2.
